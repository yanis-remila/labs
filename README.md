#  Architecture Big Data pour Gestion de Freelances


---

## RÃ‰CAPITULATIF DES 5 LIVRABLES

### âœ… 1. Diagramme de Composants

**Architecture logique en 5 couches** :

1. **Couche Ingestion**
    - File Loader JSON/CSV
    - Kafka Producer

2. **Couche Streaming**
    - **Topic: Profils** (2 partitions)
    - **Topic: Factures** (2 partitions)
    - Kafka Consumer

3. **Couche Traitement**
    - Spark Master (orchestration)
    - Worker 1 (ID 1-500)
    - Worker 2 (ID 501-1000)
    - **T1: Nettoyage** (doublons + validation)
    - **T2: Calculs Financiers** (CA + revenus)
    - **T3: AgrÃ©gations** (compÃ©tences + taux)

4. **Couche Stockage**
    - PostgreSQL (PGsql)
    - 3 tables : Freelances, Factures, Indicateurs

5. **Couche Visualisation**
    - Dashboard Financier
    - Dashboard Ressources
    - Suivi Temps RÃ©el

```mermaid
graph TB
    subgraph "ğŸ“¦ COUCHE INGESTION"
        C1["ğŸ“„ File Loader JSON/CSV<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Composant: DataIngestion<br/>RÃ´le: Lecture fichiers sources<br/>Input: JSON (Profils), CSV (Factures)"]

        C2["ğŸ”Œ Kafka Producer<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Composant: MessageBroker<br/>RÃ´le: Publication vers topics<br/>Output: 2 topics distincts"]
    end

    subgraph "ğŸš€ COUCHE STREAMING"
        C3A["ğŸ“¨ Topic: Profils<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Composant: EventStreaming<br/>RÃ´le: Flux nouveaux freelances<br/>Partitions: 2"]

        C3B["ğŸ“¨ Topic: Factures<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Composant: EventStreaming<br/>RÃ´le: Flux paiements<br/>Partitions: 2"]

        C4["ğŸ‘‚ Kafka Consumer<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Composant: StreamConsumer<br/>RÃ´le: Consommation messages<br/>Tech: Spark Streaming"]
    end

    subgraph "âš¡ COUCHE TRAITEMENT"
        C5["ğŸ¯ Spark Master<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Composant: Orchestrator<br/>RÃ´le: Coordination + Distribution<br/>API: Cluster Manager"]

        C6A["âš™ï¸ Worker 1<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Composant: DataProcessor<br/>RÃ´le: Traitement ID 1-500<br/>Partition: 0"]

        C6B["âš™ï¸ Worker 2<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Composant: DataProcessor<br/>RÃ´le: Traitement ID 501-1000<br/>Partition: 1"]

        subgraph Transform["ğŸ’¾ Transformations Spark SQL"]
            C7A["ğŸ§¹ T1: Nettoyage<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Suppression doublons<br/>Validation donnÃ©es"]

            C7B["ğŸ’° T2: Calculs Financiers<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>CA par freelance<br/>Revenus mensuels"]

            C7C["ğŸ“Š T3: AgrÃ©gations<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Top compÃ©tences<br/>Taux d'occupation"]
        end
    end

    subgraph "ğŸ—„ï¸ COUCHE STOCKAGE"
        C8["ğŸ“Š PostgreSQL (PGsql)<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Composant: DataWarehouse<br/>Tables:<br/>â€¢ Freelances (profils + compÃ©tences)<br/>â€¢ Factures (historique paiements)<br/>â€¢ Indicateurs (KPIs + stats)"]
    end

    subgraph "ğŸ“Š COUCHE VISUALISATION"
        C10A["ğŸ’° Dashboard Financier<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Composant: PowerBI-Finance<br/>â€¢ Top 10 freelances par CA<br/>â€¢ Ã‰volution revenus mensuels"]

        C10B["ğŸ‘¥ Dashboard Ressources<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Composant: PowerBI-RH<br/>â€¢ CompÃ©tences disponibles<br/>â€¢ Taux occupation: 78%"]

        C10C["âš¡ Suivi Temps RÃ©el<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Composant: PowerBI-Live<br/>â€¢ Missions en cours<br/>â€¢ Nouvelles factures"]
    end

    subgraph "ğŸ”§ COUCHE SUPPORT"
        C12["ğŸ“ Logger<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Composant: Monitoring<br/>RÃ´le: Logs & Alertes"]
    end

    C1 -->|"JSON + CSV"| C2
    C2 -->|"Produit profils"| C3A
    C2 -->|"Produit factures"| C3B
    C3A -->|"Consomme"| C4
    C3B -->|"Consomme"| C4
    C4 -->|"Envoie flux"| C5
    C5 -->|"Distribue 1-500"| C6A
    C5 -->|"Distribue 501-1000"| C6B
    C6A -->|"Process"| C7A
    C6B -->|"Process"| C7A
    C7A -->|"DonnÃ©es nettoyÃ©es"| C7B
    C7B -->|"Calculs terminÃ©s"| C7C
    C7C -->|"Ã‰crit"| C8
    C8 -->|"Connexion"| C10A
    C8 -->|"Connexion"| C10B
    C8 -->|"Connexion"| C10C
    C5 -.->|"Interroge localisation"| C6A
    C5 -.->|"Interroge localisation"| C6B
    C7A -.->|"Log"| C12
    C7B -.->|"Log"| C12
    C7C -.->|"Log"| C12

    style C1 fill:#e3f2fd
    style C2 fill:#e3f2fd
    style C3A fill:#fff9c4
    style C3B fill:#fff9c4
    style C4 fill:#fff9c4
    style C5 fill:#ef5350,color:#fff
    style C6A fill:#ffcdd2
    style C6B fill:#ffcdd2
    style C7A fill:#fff176
    style C7B fill:#fff176
    style C7C fill:#fff176
    style C8 fill:#c8e6c9
    style C10A fill:#b39ddb
    style C10B fill:#b39ddb
    style C10C fill:#b39ddb
    style C12 fill:#e0e0e0
```



---

### âœ… 2. Diagramme de DÃ©ploiement

```mermaid
graph TB
    subgraph Cloud["â˜ï¸ INFRASTRUCTURE CLOUD (AWS / Azure)"]
        subgraph VM1["ğŸ–¥ï¸ VM-01 : Ingestion & Streaming<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Ubuntu 22.04<br/>RAM: 16 GB | CPU: 8 cores<br/>Disque: 500 GB SSD<br/>IP: 10.0.1.10"]
            D1["ğŸ³ Container: File Loader<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Image: python:3.11<br/>RAM: 2 GB | CPU: 1 core<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>RÃ´le: Lecture JSON/CSV<br/>Volume: /data/input<br/>Port: 8000"]

            D2["ğŸ³ Container: Kafka Producer<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Image: python:3.11<br/>RAM: 2 GB | CPU: 1 core<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>RÃ´le: Publication vers topics<br/>Port: 9093"]

            D3["ğŸ³ Container: Zookeeper<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Image: zookeeper:3.8<br/>RAM: 2 GB | CPU: 1 core<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Port: 2181<br/>Volume: /var/lib/zookeeper"]

            D4["ğŸ³ Container: Kafka Broker 01<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Image: confluentinc/kafka:7.5<br/>RAM: 4 GB | CPU: 2 cores<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Topics:<br/>ğŸ“¨ Profils (2 partitions)<br/>ğŸ“¨ Factures (2 partitions)<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Port: 9092<br/>Volume: /var/lib/kafka"]

            D5["ğŸ³ Container: Kafka Broker 02<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Image: confluentinc/kafka:7.5<br/>RAM: 4 GB | CPU: 2 cores<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>RÃ´le: RÃ©plication topics<br/>Port: 9094"]
        end

        subgraph VM2["ğŸ–¥ï¸ VM-02 : Traitement Spark<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Ubuntu 22.04<br/>RAM: 32 GB | CPU: 16 cores<br/>Disque: 1 TB SSD<br/>IP: 10.0.2.10"]
            D6["ğŸ³ Container: Spark Master<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Image: bitnami/spark:3.5<br/>RAM: 8 GB | CPU: 4 cores<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>RÃ´les:<br/>â€¢ Coordination workers<br/>â€¢ Consumer Kafka<br/>â€¢ Distribution tÃ¢ches<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Port: 7077, 8080<br/>Env: SPARK_MODE=master"]

            D7["ğŸ³ Container: Spark Worker 01<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Image: bitnami/spark:3.5<br/>RAM: 10 GB | CPU: 5 cores<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Traitement:<br/>â€¢ Freelances ID 1-500<br/>â€¢ Partition 0<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Port: 8081<br/>Env: SPARK_WORKER_MEMORY=10G"]

            D8["ğŸ³ Container: Spark Worker 02<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Image: bitnami/spark:3.5<br/>RAM: 10 GB | CPU: 5 cores<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Traitement:<br/>â€¢ Freelances ID 501-1000<br/>â€¢ Partition 1<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Port: 8082<br/>Env: SPARK_WORKER_MEMORY=10G"]
        end

        subgraph VM3["ğŸ–¥ï¸ VM-03 : Stockage PostgreSQL<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Ubuntu 22.04<br/>RAM: 16 GB | CPU: 8 cores<br/>Disque: 2 TB SSD<br/>IP: 10.0.3.10"]
            D9["ğŸ³ Container: PostgreSQL 15<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Image: postgres:15<br/>RAM: 12 GB | CPU: 6 cores<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Database: freelances_db<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Tables:<br/>â€¢ Freelances (profils)<br/>â€¢ Factures (paiements)<br/>â€¢ Indicateurs (KPIs)<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Port: 5432<br/>Volume: /var/lib/postgresql<br/>Backup automatique"]

            D10["ğŸ³ Container: PgAdmin<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Image: dpage/pgadmin4<br/>RAM: 1 GB | CPU: 1 core<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>RÃ´le: Interface admin BDD<br/>Port: 5050"]
        end

        subgraph VM4["ğŸ–¥ï¸ VM-04 : Monitoring<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Ubuntu 22.04<br/>RAM: 8 GB | CPU: 4 cores<br/>Disque: 500 GB SSD<br/>IP: 10.0.4.10"]
            D11["ğŸ³ Container: Grafana<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Image: grafana/grafana<br/>RAM: 2 GB | CPU: 1 core<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>RÃ´le: Dashboards monitoring<br/>Port: 3000"]

            D12["ğŸ³ Container: Prometheus<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Image: prom/prometheus<br/>RAM: 3 GB | CPU: 1 core<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>RÃ´le: MÃ©triques systÃ¨me<br/>Port: 9090"]

            D13["ğŸ³ Container: Kafka UI<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Image: provectuslabs/kafka-ui<br/>RAM: 1 GB | CPU: 1 core<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>RÃ´le: Monitoring Kafka<br/>Port: 8080"]
        end

        subgraph Storage["â˜ï¸ OBJECT STORAGE"]
            N8["ğŸ“¦ S3 / Azure Blob Storage<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>CapacitÃ©: 5 TB<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Contenu:<br/>â€¢ Fichiers sources (JSON/CSV)<br/>â€¢ Backups Docker volumes<br/>â€¢ Images Docker registry<br/>â€¢ Logs applicatifs<br/>â€¢ Snapshots VMs"]
        end
    end

    subgraph OnPrem["ğŸ¢ POSTE CLIENT"]
        N9["ğŸ’» Poste Utilisateur<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>OS: Windows 11<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Applications:<br/>â€¢ Power BI Desktop<br/>â€¢ Docker Desktop (dev)<br/>â€¢ Git + VS Code<br/>â€¢ Web Browser<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Connexion: HTTPS + VPN"]
    end

    subgraph External["ğŸŒ SOURCES EXTERNES"]
        N10["ğŸ“ DÃ©pÃ´t Fichiers<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>ğŸ“„ Profils freelances (JSON)<br/>ğŸ“Š Factures (CSV)<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Protocole: SFTP / API<br/>FrÃ©quence: Quotidien<br/>Path: /data/input"]
    end

    subgraph Registry["ğŸ³ DOCKER REGISTRY"]
        DR["Docker Hub / Private Registry<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Images personnalisÃ©es:<br/>â€¢ freelanceflow/loader:v1.0<br/>â€¢ freelanceflow/producer:v1.0<br/>â€¢ freelanceflow/spark-jobs:v1.0<br/>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€<br/>Images officielles:<br/>â€¢ postgres:15<br/>â€¢ kafka:7.5<br/>â€¢ spark:3.5<br/>â€¢ grafana, prometheus"]
    end

    N10 -->|"SFTP Upload<br/>Port 22"| D1
    D1 -->|"Fichiers traitÃ©s"| D2
    D2 -->|"Kafka Protocol<br/>Port 9092"| D4
    D3 <-->|"Coordination<br/>Zookeeper"| D4
    D4 <-->|"RÃ©plication<br/>Topics"| D5
    D4 -->|"Consumer<br/>Topics P+F"| D6

    D6 -->|"Spark RPC<br/>Distribue 1-500"| D7
    D6 -->|"Spark RPC<br/>Distribue 501-1000"| D8
    D6 -.->|"Query: DonnÃ©es<br/>freelance #245?"| D7
    D7 -.->|"Response: Oui"| D6

    D7 -->|"JDBC Write<br/>Port 5432"| D9
    D8 -->|"JDBC Write<br/>Port 5432"| D9

    D9 -->|"HTTPS<br/>DirectQuery"| N9
    D10 -.->|"Admin BDD"| D9

    D11 -->|"Scrape metrics"| D6
    D11 -->|"Scrape metrics"| D9
    D12 -->|"Push metrics"| D11
    D13 -->|"Monitor"| D4

    N9 -.->|"Monitoring UI"| D11
    N9 -.->|"Spark UI"| D6
    N9 -.->|"Kafka UI"| D13

    D1 -.->|"Backup"| N8
    D4 -.->|"Backup"| N8
    D9 -.->|"Backup"| N8

    DR -.->|"Pull images"| VM1
    DR -.->|"Pull images"| VM2
    DR -.->|"Pull images"| VM3
    DR -.->|"Pull images"| VM4

    style Cloud fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style VM1 fill:#fff3e0,stroke:#e65100,stroke-width:3px
    style VM2 fill:#ffebee,stroke:#c62828,stroke-width:4px
    style VM3 fill:#e8f5e9,stroke:#2e7d32,stroke-width:3px
    style VM4 fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px
    style Storage fill:#e0f2f1,stroke:#00695c,stroke-width:2px
    style OnPrem fill:#b39ddb,stroke:#5e35b1,stroke-width:2px
    style External fill:#e0e0e0,stroke:#616161,stroke-width:2px
    style Registry fill:#bbdefb,stroke:#1976d2,stroke-width:2px

    style D1 fill:#42a5f5,color:#fff
    style D2 fill:#42a5f5,color:#fff
    style D3 fill:#81c784
    style D4 fill:#ffd54f,stroke:#f57f17,stroke-width:2px
    style D5 fill:#ffd54f
    style D6 fill:#ef5350,color:#fff,stroke:#c62828,stroke-width:3px
    style D7 fill:#ff8a80
    style D8 fill:#ff8a80
    style D9 fill:#66bb6a,stroke:#2e7d32,stroke-width:3px
    style D10 fill:#aed581
    style D11 fill:#ba68c8,color:#fff
    style D12 fill:#ce93d8
    style D13 fill:#f48fb1
```

---

### âœ… 3. Indicateurs BI et Machine Learning

#### ğŸ“Š 3 Dashboards Power BI

**1. Dashboard Financier**
- Top 10 freelances par CA
- Ã‰volution revenus mensuels (line chart)
- KPIs : CA total, TJM moyen
- RequÃªtes SQL : CA par freelance, Revenus mensuels

**2. Dashboard Ressources**
- CompÃ©tences disponibles (pie chart)
- Taux d'occupation : 78% (gauge)
- Freelances disponibles vs en mission
- Top compÃ©tences demandÃ©es

**3. Suivi Temps RÃ©el**
- Missions en cours (compteur live)
- Nouvelles factures du jour
- Refresh : 30 secondes
- Alertes factures en retard

#### ğŸ¤– 3 ModÃ¨les ML

**1. PrÃ©diction CA Mensuel**
- Type : Time Series (ARIMA/Prophet)
- MAPE cible : < 15%
- PrÃ©diction : 3 mois

**2. Matching Freelance-CompÃ©tence**
- Type : Content-Based Filtering
- Precision@5 : > 70%

**3. DÃ©tection Anomalies Factures**
- Type : Isolation Forest
- Taux faux positifs : < 5%

#### ğŸ’¾ 3 Transformations Spark SQL

**T1: Nettoyage**
```sql
-- Suppression doublons
-- Validation formats (email, dates, montants)
-- Normalisation donnÃ©es
```

**T2: Calculs Financiers**
```sql
-- CA par freelance
-- Revenus mensuels
-- TJM moyen par compÃ©tence
```

**T3: AgrÃ©gations**
```sql
-- Top compÃ©tences demandÃ©es
-- Taux d'occupation global
-- KPIs consolidÃ©s
```

---

### âœ… 4. Backlog Teams


#### ğŸ“‹ FreelanceFlow - Backlog Projet (Version Finale)

## ğŸ¯ EPIC 1 : Infrastructure Kafka & Ingestion

### ğŸ“¦ User Story 1.1 : Configuration Kafka avec 2 Topics
**En tant que** Data Engineer  
**Je veux** configurer un cluster Kafka avec 2 topics distincts  
**Afin de** sÃ©parer les flux Profils et Factures

**CritÃ¨res d'acceptation** :
- [ ] Cluster Kafka dÃ©ployÃ© avec 2 brokers
- [ ] **Topic "Profils"** crÃ©Ã© avec 2 partitions
- [ ] **Topic "Factures"** crÃ©Ã© avec 2 partitions
- [ ] RÃ©plication factor = 2 pour chaque topic
- [ ] Tests production/consommation sur les 2 topics
- [ ] Monitoring topics dans Kafka UI

**PrioritÃ©** : ğŸ”´ Haute  
**Points de complexitÃ©** : 8  
**Sprint** : Sprint 1  
**AssignÃ© Ã ** : @DataEngineering  
**Tags** : `infrastructure` `kafka` `topics` `streaming`

---

### ğŸ“¦ User Story 1.2 : Script ingestion JSON/CSV vers Kafka
**En tant que** Data Engineer  
**Je veux** crÃ©er un producer Kafka qui lit JSON et CSV  
**Afin de** publier vers les bons topics

**CritÃ¨res d'acceptation** :
- [ ] Lecture fichiers JSON (profils freelances)
- [ ] Lecture fichiers CSV (factures)
- [ ] Publication JSON â†’ Topic "Profils"
- [ ] Publication CSV â†’ Topic "Factures"
- [ ] Chargement quotidien automatisÃ©
- [ ] Logs des publications
- [ ] Gestion erreurs et retry

**PrioritÃ©** : ğŸ”´ Haute  
**Points de complexitÃ©** : 5  
**Sprint** : Sprint 1  
**AssignÃ© Ã ** : @Developer1  
**Tags** : `python` `kafka-producer` `json` `csv`

---

## âš¡ EPIC 2 : Cluster Spark & Distribution

### ğŸ“¦ User Story 2.1 : DÃ©ploiement Cluster Spark
**En tant que** Data Engineer  
**Je veux** dÃ©ployer 1 master + 2 workers  
**Afin de** traiter les donnÃ©es en parallÃ¨le

**CritÃ¨res d'acceptation** :
- [ ] Spark Master configurÃ© (32 GB RAM, 16 cores)
- [ ] Worker 1 : traitement ID 1-500 (16 GB RAM)
- [ ] Worker 2 : traitement ID 501-1000 (16 GB RAM)
- [ ] Interface Web Spark accessible
- [ ] Tests de distribution fonctionnels
- [ ] Configuration mÃ©moire optimisÃ©e

**PrioritÃ©** : ğŸ”´ Haute  
**Points de complexitÃ©** : 8  
**Sprint** : Sprint 1  
**AssignÃ© Ã ** : @DevOps  
**Tags** : `spark` `infrastructure` `distributed`

---

### ğŸ“¦ User Story 2.2 : Consumer Kafka vers Spark
**En tant que** Data Engineer  
**Je veux** consommer les 2 topics Kafka avec Spark Streaming  
**Afin de** alimenter le traitement distribuÃ©

**CritÃ¨res d'acceptation** :
- [ ] Consumer Spark Streaming fonctionnel
- [ ] Lecture Topic "Profils"
- [ ] Lecture Topic "Factures"
- [ ] Transformation en DataFrame Spark
- [ ] Distribution vers workers appropriÃ©s
- [ ] Gestion offsets Kafka
- [ ] Tests d'intÃ©gration

**PrioritÃ©** : ğŸ”´ Haute  
**Points de complexitÃ©** : 13  
**Sprint** : Sprint 2  
**AssignÃ© Ã ** : @Developer2  
**Tags** : `spark-streaming` `kafka-consumer` `dataframe`

---

## ğŸ’¾ EPIC 3 : Transformations Spark SQL

### ğŸ“¦ User Story 3.1 : T1 - Nettoyage des donnÃ©es
**En tant que** Data Analyst  
**Je veux** implÃ©menter le module de nettoyage  
**Afin de** supprimer doublons et valider les donnÃ©es

**CritÃ¨res d'acceptation** :
- [ ] Suppression doublons (profils + factures)
- [ ] Validation emails (format correct)
- [ ] Validation montants (> 0)
- [ ] Validation dates (format ISO)
- [ ] Gestion valeurs nulles
- [ ] Logs des rejets
- [ ] MÃ©triques qualitÃ© donnÃ©es

**PrioritÃ©** : ğŸ”´ Haute  
**Points de complexitÃ©** : 5  
**Sprint** : Sprint 2  
**AssignÃ© Ã ** : @DataAnalyst  
**Tags** : `spark-sql` `data-quality` `cleaning`

---

### ğŸ“¦ User Story 3.2 : T2 - Calculs Financiers
**En tant que** Data Analyst  
**Je veux** calculer les mÃ©triques financiÃ¨res  
**Afin de** obtenir CA par freelance et revenus mensuels

**CritÃ¨res d'acceptation** :
- [ ] Calcul CA par freelance
- [ ] Calcul revenus mensuels
- [ ] TJM moyen par compÃ©tence
- [ ] AgrÃ©gation par pÃ©riode
- [ ] RequÃªtes SQL optimisÃ©es (< 5s)
- [ ] Tests unitaires calculs

**PrioritÃ©** : ğŸ”´ Haute  
**Points de complexitÃ©** : 8  
**Sprint** : Sprint 2  
**AssignÃ© Ã ** : @DataAnalyst  
**Tags** : `spark-sql` `financial` `kpi`

---

### ğŸ“¦ User Story 3.3 : T3 - AgrÃ©gations
**En tant que** Data Analyst  
**Je veux** calculer les agrÃ©gations mÃ©tier  
**Afin d'** obtenir top compÃ©tences et taux d'occupation

**CritÃ¨res d'acceptation** :
- [ ] Top compÃ©tences demandÃ©es
- [ ] Taux d'occupation : 78% (target)
- [ ] Statistiques par catÃ©gorie
- [ ] Freelances disponibles par compÃ©tence
- [ ] RequÃªtes avec window functions
- [ ] Performance < 3s

**PrioritÃ©** : ğŸŸ¡ Moyenne  
**Points de complexitÃ©** : 8  
**Sprint** : Sprint 3  
**AssignÃ© Ã ** : @DataAnalyst  
**Tags** : `spark-sql` `aggregation` `analytics`

---

## ğŸ—„ï¸ EPIC 4 : Data Warehouse PostgreSQL

### ğŸ“¦ User Story 4.1 : CrÃ©ation schÃ©ma PostgreSQL (PGsql)
**En tant que** Data Architect  
**Je veux** crÃ©er le schÃ©ma de base PostgreSQL  
**Afin de** stocker les 3 tables principales

**CritÃ¨res d'acceptation** :
- [ ] Base `freelances_db` crÃ©Ã©e
- [ ] Table `Freelances` (profils + compÃ©tences)
- [ ] Table `Factures` (historique paiements)
- [ ] Table `Indicateurs` (KPIs + statistiques)
- [ ] Index optimisÃ©s
- [ ] Contraintes d'intÃ©gritÃ©
- [ ] Documentation schÃ©ma

**PrioritÃ©** : ğŸ”´ Haute  
**Points de complexitÃ©** : 5  
**Sprint** : Sprint 2  
**AssignÃ© Ã ** : @DataArchitect  
**Tags** : `postgresql` `pgsql` `schema` `database`

---

### ğŸ“¦ User Story 4.2 : Pipeline ETL Spark â†’ PostgreSQL
**En tant que** Data Engineer  
**Je veux** Ã©crire les donnÃ©es transformÃ©es dans PostgreSQL  
**Afin de** persister les rÃ©sultats des transformations T1, T2, T3

**CritÃ¨res d'acceptation** :
- [ ] Connexion JDBC Spark â†’ PostgreSQL
- [ ] Ã‰criture table Freelances
- [ ] Ã‰criture table Factures
- [ ] Ã‰criture table Indicateurs
- [ ] Mode upsert (insert/update)
- [ ] Transactions ACID
- [ ] Tests de charge

**PrioritÃ©** : ğŸ”´ Haute  
**Points de complexitÃ©** : 8  
**Sprint** : Sprint 3  
**AssignÃ© Ã ** : @Developer1  
**Tags** : `etl` `jdbc` `postgresql` `pgsql`

---

## ğŸ“Š EPIC 5 : Dashboards Power BI

### ğŸ“¦ User Story 5.1 : Dashboard Financier
**En tant que** Directeur Financier  
**Je veux** visualiser le Top 10 et l'Ã©volution des revenus  
**Afin de** suivre la performance financiÃ¨re

**CritÃ¨res d'acceptation** :
- [ ] **Top 10 freelances par CA** : graphique bar chart
- [ ] **Ã‰volution revenus mensuels** : line chart 12 mois
- [ ] Filtres : pÃ©riode, compÃ©tence
- [ ] KPI : CA total, TJM moyen
- [ ] Refresh quotidien automatique
- [ ] Export Excel

**PrioritÃ©** : ğŸ”´ Haute  
**Points de complexitÃ©** : 5  
**Sprint** : Sprint 3  
**AssignÃ© Ã ** : @BIAnalyst  
**Tags** : `powerbi` `dashboard` `finance`

---

### ğŸ“¦ User Story 5.2 : Dashboard Ressources
**En tant que** Responsable RH  
**Je veux** voir les compÃ©tences disponibles et le taux d'occupation  
**Afin d'** optimiser l'allocation des freelances

**CritÃ¨res d'acceptation** :
- [ ] **CompÃ©tences disponibles** : pie chart par techno
- [ ] **Taux d'occupation : 78%** : gauge visual
- [ ] RÃ©partition freelances disponibles/en mission
- [ ] Filtres par compÃ©tence
- [ ] Alerte si taux < 60% ou > 90%
- [ ] DÃ©tail freelances disponibles (table)

**PrioritÃ©** : ğŸ”´ Haute  
**Points de complexitÃ©** : 5  
**Sprint** : Sprint 4  
**AssignÃ© Ã ** : @BIAnalyst  
**Tags** : `powerbi` `dashboard` `hr` `resources`

---

### ğŸ“¦ User Story 5.3 : Suivi Temps RÃ©el
**En tant que** Manager OpÃ©rationnel  
**Je veux** voir les missions en cours et nouvelles factures  
**Afin de** piloter l'activitÃ© en temps rÃ©el

**CritÃ¨res d'acceptation** :
- [ ] **Missions en cours** : compteur + liste
- [ ] **Nouvelles factures** : compteur du jour
- [ ] Refresh toutes les 30 secondes
- [ ] DerniÃ¨res 10 factures crÃ©Ã©es (table)
- [ ] Alertes : factures en retard
- [ ] Indicateur statut systÃ¨me (vert/orange/rouge)

**PrioritÃ©** : ğŸŸ¡ Moyenne  
**Points de complexitÃ©** : 8  
**Sprint** : Sprint 5  
**AssignÃ© Ã ** : @BIAnalyst  
**Tags** : `powerbi` `realtime` `streaming` `monitoring`

---

## ğŸ¤– EPIC 6 : Machine Learning

### ğŸ“¦ User Story 6.1 : ModÃ¨le prÃ©diction CA
**En tant que** Data Scientist  
**Je veux** crÃ©er un modÃ¨le Time Series pour prÃ©dire le CA  
**Afin d'** anticiper les revenus des 3 prochains mois

**CritÃ¨res d'acceptation** :
- [ ] Collecte historique CA mensuel (12+ mois)
- [ ] Feature engineering (saisonnalitÃ©, tendances)
- [ ] EntraÃ®nement modÃ¨le (ARIMA/Prophet)
- [ ] MAPE < 15%
- [ ] PrÃ©dictions Ã  3 mois
- [ ] IntÃ©gration dans Dashboard Financier
- [ ] Documentation modÃ¨le

**PrioritÃ©** : ğŸŸ¡ Moyenne  
**Points de complexitÃ©** : 13  
**Sprint** : Sprint 6  
**AssignÃ© Ã ** : @DataScientist  
**Tags** : `ml` `timeseries` `forecasting` `arima`

---

### ğŸ“¦ User Story 6.2 : DÃ©tection anomalies factures
**En tant que** ContrÃ´leur Financier  
**Je veux** dÃ©tecter automatiquement les factures suspectes  
**Afin de** prÃ©venir les fraudes

**CritÃ¨res d'acceptation** :
- [ ] Algorithme Isolation Forest
- [ ] Features : montant, frÃ©quence, TJM
- [ ] Taux faux positifs < 5%
- [ ] IntÃ©gration Suivi Temps RÃ©el
- [ ] Alertes automatiques
- [ ] Dashboard anomalies dÃ©tectÃ©es

**PrioritÃ©** : ğŸŸ¢ Basse  
**Points de complexitÃ©** : 13  
**Sprint** : Sprint 7  
**AssignÃ© Ã ** : @DataScientist  
**Tags** : `ml` `anomaly-detection` `fraud`

---

## ğŸ”§ EPIC 7 : DevOps & Monitoring

### ğŸ“¦ User Story 7.1 : CI/CD Pipeline
**En tant que** DevOps Engineer  
**Je veux** automatiser le dÃ©ploiement  
**Afin d'** accÃ©lÃ©rer les mises en production

**CritÃ¨res d'acceptation** :
- [ ] Pipeline GitHub Actions / GitLab CI
- [ ] Tests automatisÃ©s (unit + integration)
- [ ] Build Docker images (Kafka, Spark)
- [ ] DÃ©ploiement auto sur dev
- [ ] Validation manuelle pour prod
- [ ] Rollback automatique si erreur

**PrioritÃ©** : ğŸŸ¡ Moyenne  
**Points de complexitÃ©** : 8  
**Sprint** : Sprint 3  
**AssignÃ© Ã ** : @DevOps  
**Tags** : `cicd` `automation` `deployment` `docker`

---

### ğŸ“¦ User Story 7.2 : Monitoring systÃ¨me
**En tant que** Ops Engineer  
**Je veux** monitorer la santÃ© de l'infrastructure  
**Afin de** dÃ©tecter les problÃ¨mes rapidement

**CritÃ¨res d'acceptation** :
- [ ] Monitoring Kafka (lag, throughput)
- [ ] Monitoring Spark (jobs, stages)
- [ ] Monitoring PostgreSQL (connexions, queries)
- [ ] MÃ©triques systÃ¨me (CPU, RAM, Disk)
- [ ] Alertes emails/Slack
- [ ] Dashboard Grafana
- [ ] SLA : uptime > 99%

**PrioritÃ©** : ğŸŸ¡ Moyenne  
**Points de complexitÃ©** : 8  
**Sprint** : Sprint 4  
**AssignÃ© Ã ** : @DevOps  
**Tags** : `monitoring` `grafana` `alerting` `sla`

---

## ğŸ” ARCHITECTURE DÃ‰TAILLÃ‰E

### ğŸ“¥ Sources de DonnÃ©es
- **Fichiers JSON** : Profils freelances
  ```json
  {"nom": "Dupont", "compÃ©tences": ["Python"], "tarif_jour": "450â‚¬"}
  ```
- **Fichiers CSV** : Factures
  ```
  freelance, montant, date
  Dupont, 4500â‚¬, Oct-2025
  ```

### ğŸš€ Kafka - 2 Topics
1. **Topic: Profils**
    - ReÃ§oit les nouveaux freelances
    - 2 partitions
    - RÃ©plication factor = 2

2. **Topic: Factures**
    - ReÃ§oit les paiements
    - 2 partitions
    - RÃ©plication factor = 2

### âš¡ Spark - Architecture DistribuÃ©e

**NÅ“ud MaÃ®tre** :
- Coordonne le travail
- Distribue les tÃ¢ches
- Interroge les workers : *"Tu as les donnÃ©es du freelance #245 ?"*

**Worker 1** :
- Traite freelances ID 1-500
- Partition 0

**Worker 2** :
- Traite freelances ID 501-1000
- Partition 1

### ğŸ’¾ 3 Transformations Spark SQL

**T1 : Nettoyage**
- Supprime doublons
- Valide les donnÃ©es

**T2 : Calculs Financiers**
- CA par freelance
- Revenus mensuels

**T3 : AgrÃ©gations**
- Top compÃ©tences demandÃ©es
- Taux d'occupation

### ğŸ—„ï¸ PostgreSQL (PGsql)

**Base : freelances_db**

**3 Tables** :
1. **Freelances** : Profils et compÃ©tences
2. **Factures** : Historique paiements
3. **Indicateurs** : KPIs et statistiques

### ğŸ“Š Power BI - 3 Dashboards

**1. Dashboard Financier**
- Top 10 freelances par CA
- Ã‰volution revenus mensuels

**2. Dashboard Ressources**
- CompÃ©tences disponibles
- Taux d'occupation : 78%

**3. Suivi Temps RÃ©el**
- Missions en cours
- Nouvelles factures

---
